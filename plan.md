# PR-Scoped Canary Deployments with Kustomize, Traefik, Linkerd & Argo CD

> Test new versions **per Pull Request** in a real cluster without disrupting others.
> Requests tagged with `X-Canary: <PR#>` go to canary version, everything else goes to stable.
> Auto-created on PR open, auto-pruned on PR close.

---

## Table of Contents

* [Why this approach](#why-this-approach)
* [Architecture](#architecture)
* [Prerequisites](#prerequisites)
* [Repository Structure](#repository-structure)
* [Service Architecture](#service-architecture)
* [Kustomize Configuration](#kustomize-configuration)
* [Traefik Routing](#traefik-routing)
* [Linkerd Routing](#linkerd-routing)
* [Argo CD ApplicationSets](#argo-cd-applicationsets)
* [GitHub Actions](#github-actions)
* [Observability Stack](#observability-stack)
* [Local Development](#local-development)
* [Promotion & Cleanup Lifecycle](#promotion--cleanup-lifecycle)
* [Troubleshooting](#troubleshooting)

---

## Why this approach

* **Fast feedback** â€” Every PR deploys a live, isolated canary you can hit immediately.
* **Zero disruption** â€” Only requests with `X-Canary: <PR#>` hit the PR namespace; everyone else stays on stable.
* **Production parity** â€” Test in the *real* cluster with real dependencies.
* **Automatic cleanup** â€” When the PR closes, Argo CD prunes the entire PR namespace automatically.
* **GitOps alignment** â€” Everything is declarative, auditable, reproducible.
* **Easy demos** â€” Share the header value and a URL; no bespoke staging envs.
* **Seamless promotion** â€” Release is just updating the stable image tag.
* **Single source of truth** â€” Same base manifests deployed everywhere, no v1/v2 duplication.

---

## Architecture

* **Web â†’ API (HTTP)** routed by **Traefik**:
  * `X-Canary: <PR#>` â†’ **`api-service`** in **`api-canary-pr-<PR#>`** namespace
  * otherwise â†’ **`api-service`** in **`apps`** namespace (stable)

* **API â†’ Store (gRPC)** routed by **Linkerd (Gateway API)**:
  * Same `X-Canary` value propagated as **gRPC metadata**.
  * `X-Canary: <PR#>` â†’ **`store-service`** in **`store-canary-pr-<PR#>`** namespace
  * otherwise â†’ **`store-service`** in **`apps`** namespace (stable)

> You'll run **the same base manifests** in different namespaces per PR.
> Cross-namespace routing via GRPCRoute + ReferenceGrant enables east-west traffic.

---

## Prerequisites

* Kubernetes cluster with:
  * **Argo CD** + **ApplicationSet** controller
  * **Traefik** (CRDs installed for `IngressRoute`)
  * **Linkerd** with **Gateway API dynamic request routing** enabled
* Container registry (GHCR) and CI secrets to push images
* DNS for your dev domain (e.g., `api.localhost`)
* GitHub token (PAT) for ApplicationSet PR generator

---

## Repository Structure

### Current Implementation

```
.
â”œâ”€â”€ api-service/                    # HTTP REST API service
â”‚   â”œâ”€â”€ main.go                     # Application entry point
â”‚   â”œâ”€â”€ internal/                   # Internal packages
â”‚   â”‚   â”œâ”€â”€ client/                 # gRPC client for store-service
â”‚   â”‚   â”œâ”€â”€ server/                 # HTTP handlers and mappers
â”‚   â”‚   â”œâ”€â”€ canaryctx/              # Canary context handling
â”‚   â”‚   â”œâ”€â”€ metrics/                # Prometheus metrics
â”‚   â”‚   â””â”€â”€ tracing/                # OpenTelemetry tracing
â”‚   â”œâ”€â”€ .github/workflows/          # CI/CD workflows
â”‚   â”œâ”€â”€ .gitignore                  # Git ignore rules
â”‚   â”œâ”€â”€ go.mod                      # Go dependencies
â”‚   â”œâ”€â”€ Makefile                    # Build and development tasks
â”‚   â”œâ”€â”€ Dockerfile                  # Container image
â”‚   â””â”€â”€ README.md                   # Service documentation
â”‚
â”œâ”€â”€ store-service/                  # gRPC backend service
â”‚   â”œâ”€â”€ main.go                     # Application entry point
â”‚   â”œâ”€â”€ internal/                   # Internal packages
â”‚   â”‚   â”œâ”€â”€ data/                   # DynamoDB data layer
â”‚   â”‚   â”œâ”€â”€ server/                 # gRPC server implementation
â”‚   â”‚   â”œâ”€â”€ canaryctx/              # Canary context handling
â”‚   â”‚   â”œâ”€â”€ metrics/                # Prometheus metrics
â”‚   â”‚   â””â”€â”€ tracing/                # OpenTelemetry tracing
â”‚   â”œâ”€â”€ proto/                      # Protocol buffer definitions
â”‚   â”‚   â”œâ”€â”€ store.proto             # Service definition
â”‚   â”‚   â”œâ”€â”€ generate.go             # Code generation
â”‚   â”‚   â”œâ”€â”€ go/                     # Generated Go code
â”‚   â”‚   â””â”€â”€ ruby/                   # Ruby gem structure
â”‚   â”œâ”€â”€ .github/workflows/          # CI/CD workflows
â”‚   â”œâ”€â”€ .gitignore                  # Git ignore rules
â”‚   â”œâ”€â”€ go.mod                      # Go dependencies
â”‚   â”œâ”€â”€ Makefile                    # Build and development tasks
â”‚   â”œâ”€â”€ Dockerfile                  # Container image
â”‚   â””â”€â”€ README.md                   # Service documentation
â”‚
â”œâ”€â”€ manifests-microservices/        # Centralized manifests repository
â”‚   â”œâ”€â”€ applications/               # Argo CD applications
â”‚   â”‚   â”œâ”€â”€ production/             # Production environment
â”‚   â”‚   â”œâ”€â”€ staging/                # Staging environment
â”‚   â”‚   â””â”€â”€ integration-001/        # Integration environment
â”‚   â”œâ”€â”€ kustomize/                  # Kustomize configurations
â”‚   â”‚   â”œâ”€â”€ api-service/            # API service manifests
â”‚   â”‚   â”‚   â”œâ”€â”€ base/               # Base manifests
â”‚   â”‚   â”‚   â””â”€â”€ overlays/           # Environment overlays
â”‚   â”‚   â””â”€â”€ store-service/          # Store service manifests
â”‚   â”‚       â”œâ”€â”€ base/               # Base manifests
â”‚   â”‚       â””â”€â”€ overlays/           # Environment overlays
â”‚   â””â”€â”€ .github/workflows/          # Manifest update workflows
â”‚
â”œâ”€â”€ localdev/                       # Local development setup
â”‚   â”œâ”€â”€ infrastructure/             # Helm charts and values
â”‚   â”œâ”€â”€ scripts/                    # Helper scripts
â”‚   â”œâ”€â”€ docs/                       # Development documentation
â”‚   â”œâ”€â”€ Makefile                    # Local development tasks
â”‚   â””â”€â”€ setup.sh                    # Environment setup
â”‚
â””â”€â”€ extension-canary/               # Chrome extension for testing
    â”œâ”€â”€ manifest.json               # Extension manifest
    â”œâ”€â”€ src/                        # Extension source code
    â””â”€â”€ README.md                   # Extension documentation
```

---

## Service Architecture

### API Service (HTTP Edge)
- **Language**: Go
- **Framework**: Standard library with custom HTTP handlers
- **Protocol**: HTTP REST API
- **Database**: None (proxies to store-service)
- **Observability**: Prometheus metrics, OpenTelemetry tracing
- **Port**: 8080 (HTTP), 9090 (metrics)

### Store Service (gRPC Backend)
- **Language**: Go
- **Framework**: gRPC
- **Protocol**: gRPC
- **Database**: DynamoDB
- **Observability**: Prometheus metrics, OpenTelemetry tracing
- **Port**: 8080 (gRPC), 9090 (metrics)

---

## Kustomize Configuration

### Centralized Manifests
All Kubernetes manifests are centralized in the `manifests-microservices` repository:

```
manifests-microservices/
â”œâ”€â”€ kustomize/
â”‚   â”œâ”€â”€ api-service/
â”‚   â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ hpa.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ servicemonitor.yaml
â”‚   â”‚   â”‚   â””â”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ overlays/
â”‚   â”‚       â”œâ”€â”€ production/
â”‚   â”‚       â”œâ”€â”€ staging/
â”‚   â”‚       â””â”€â”€ integration-001/
â”‚   â””â”€â”€ store-service/
â”‚       â”œâ”€â”€ base/
â”‚       â”‚   â”œâ”€â”€ deployment.yaml
â”‚       â”‚   â”œâ”€â”€ service.yaml
â”‚       â”‚   â”œâ”€â”€ hpa.yaml
â”‚       â”‚   â”œâ”€â”€ servicemonitor.yaml
â”‚       â”‚   â”œâ”€â”€ grpc-route-template.yaml
â”‚       â”‚   â”œâ”€â”€ reference-grant-template.yaml
â”‚       â”‚   â””â”€â”€ kustomization.yaml
â”‚       â””â”€â”€ overlays/
â”‚           â”œâ”€â”€ production/
â”‚           â”œâ”€â”€ staging/
â”‚           â””â”€â”€ integration-001/
```

### Single Base Manifest Approach
- **One deployment.yaml per service** - no v1/v2 duplication
- **Environment-specific overlays** - patch image tags and configurations
- **PR canaries** - ApplicationSets patch image tags dynamically

---

## Traefik Routing

### Stable Routing
```yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: api-service-stable
  namespace: apps
spec:
  entryPoints: [web]
  routes:
    - match: Host(`api.localhost`) && PathPrefix(`/`)
      services:
        - name: api-service
          namespace: apps
          port: 80
```

### PR Canary Routing
```yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: api-service-canary-pr-{{ number }}
  namespace: apps
spec:
  entryPoints: [web]
  routes:
    - match: Host(`api.localhost`) && Headers(`X-Canary`, `{{ number }}`)
      services:
        - name: api-service
          namespace: api-canary-pr-{{ number }}
          port: 80
```

---

## Linkerd Routing

### Cross-Namespace gRPC Routing
```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: GRPCRoute
metadata:
  name: store-service-canary-pr-{{ number }}
  namespace: apps
spec:
  parentRefs:
    - kind: Service
      name: store-service
  rules:
    - matches:
        - headers:
            - name: X-Canary
              value: "{{ number }}"
      backendRefs:
        - name: store-service
          namespace: store-canary-pr-{{ number }}
          port: 80
    - backendRefs:
        - name: store-service
          namespace: apps
          port: 80
```

### Reference Grant
```yaml
apiVersion: gateway.networking.k8s.io/v1beta1
kind: ReferenceGrant
metadata:
  name: allow-apps-grpcroute
  namespace: store-canary-pr-{{ number }}
spec:
  from:
    - group: gateway.networking.k8s.io
      kind: GRPCRoute
      namespace: apps
  to:
    - group: ""
      kind: Service
      name: store-service
```

---

## Argo CD ApplicationSets

### Environment-Centric Structure
```
manifests-microservices/
â”œâ”€â”€ applications/
â”‚   â”œâ”€â”€ production/
â”‚   â”‚   â”œâ”€â”€ api-service-production.yaml
â”‚   â”‚   â””â”€â”€ store-service-production.yaml
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ api-service-staging.yaml
â”‚   â”‚   â””â”€â”€ store-service-staging.yaml
â”‚   â””â”€â”€ integration-001/
â”‚       â”œâ”€â”€ api-service-integration.yaml
â”‚       â”œâ”€â”€ store-service-integration.yaml
â”‚       â”œâ”€â”€ applicationset-api-service-pr.yaml
â”‚       â”œâ”€â”€ applicationset-store-service-pr.yaml
â”‚       â””â”€â”€ applicationset-store-service-routing-pr.yaml
```

### PR Canary ApplicationSets
- **API Service**: Deploys base manifests to PR namespace
- **Store Service**: Deploys base manifests to PR namespace  
- **Store Routing**: Creates GRPCRoute + ReferenceGrant for cross-namespace access

---

## GitHub Actions

### Generic Workflows
All workflows use `GITHUB_REPOSITORY` environment variable for dynamic naming:

```yaml
env:
  GO_VERSION: '1.25.1'
  DOCKER_IMAGE: ghcr.io/${{ github.repository }}
  REGISTRY: ghcr.io
```

### Workflow Types
1. **CI Pipeline** - Build and test on PR, build and push on main
2. **PR Canary** - Build canary image for PR testing
3. **Release** - Create staging/production PRs and GitHub releases

### Release Process
1. **Tag creation** triggers release workflow
2. **Integration** automatically updated to latest
3. **Staging PR** created for review
4. **Production PR** created for review
5. **GitHub Release** created with artifacts

---

## Observability Stack

### Local Development
- **Prometheus** - Metrics collection
- **Grafana** - Visualization and dashboards
- **Tempo** - Distributed tracing backend
- **Loki** - Log aggregation
- **Correlation** - Traces, logs, and metrics linked

### Application Metrics
- **HTTP metrics** - Request rate, latency, error rate
- **gRPC metrics** - Call rate, latency, error rate
- **Business metrics** - Custom application metrics
- **ServiceMonitor** - Prometheus scraping configuration

### Distributed Tracing
- **OpenTelemetry** - Instrumentation and export
- **Automatic HTTP tracing** - Request/response spans
- **Automatic gRPC tracing** - Client/server spans
- **Custom business spans** - Application-specific tracing
- **Trace propagation** - Headers and metadata

---

## Local Development

### Prerequisites
- **Docker Desktop** with Kubernetes enabled
- **minikube** (8 CPUs, 32GB RAM)
- **kubectl**, **helm**, **argocd** CLI
- **telepresence** for local development

### Setup
```bash
# Start minikube with sufficient resources
minikube start --memory=32768 --cpus=8

# Deploy infrastructure
cd localdev
make infrastructure

# Setup ArgoCD applications
make argocd-setup

# Start port forwards
make port-forwards
```

### Development Workflow
```bash
# Intercept service for local development
telepresence intercept api-service --namespace apps --port 8080

# Run local service
make dev-run

# Test canary deployment
curl -H "X-Canary: 123" http://api.localhost/health
```

---

## Promotion & Cleanup Lifecycle

### PR Lifecycle
1. **PR Created** â†’ GitHub Actions builds `pr-{{ number }}` image
2. **ApplicationSet Triggers** â†’ Deploys to `<service>-canary-pr-<PR#>` namespace
3. **Routing Updates** â†’ Traefik/Linkerd route `X-Canary: <PR#>` to canary
4. **Testing** â†’ Use Chrome extension or curl with header
5. **PR Updated** â†’ New image built, same namespace updated
6. **PR Closed** â†’ Argo CD prunes namespace automatically

### Release Lifecycle
1. **Tag Created** â†’ Release workflow triggered
2. **Integration Updated** â†’ Automatically updated to latest
3. **Staging PR** â†’ Created for review and testing
4. **Production PR** â†’ Created for review and approval
5. **GitHub Release** â†’ Created with release notes and artifacts

---

## Troubleshooting

### Common Issues
- **Header present but API still stable** - Check Traefik IngressRoute match
- **API routes to canary but backend still stable** - Verify gRPC interceptors
- **CORS errors** - Add `Access-Control-Allow-Headers: X-Canary`
- **ApplicationSet didn't create PR app** - Check PAT and repo permissions
- **No auto-cleanup** - Ensure `automated.prune: true` in ApplicationSet

### Debugging Commands
```bash
# Check ApplicationSet status
kubectl get applicationset -n argocd

# Check PR applications
kubectl get applications -n argocd | grep canary

# Check routing
kubectl get ingressroute -n apps
kubectl get grpcroute -n apps

# Check canary pods
kubectl get pods -n api-canary-pr-123
kubectl get pods -n store-canary-pr-123
```

---

## Current Implementation Status

### âœ… Completed
- **Service Architecture** - API and Store services with proper separation
- **Centralized Manifests** - Single source of truth in manifests-microservices
- **PR Canary Deployments** - Full ApplicationSet automation
- **Observability Stack** - Prometheus, Grafana, Tempo, Loki
- **Local Development** - Complete minikube setup with Telepresence
- **CI/CD Pipelines** - Generic GitHub Actions workflows
- **Chrome Extension** - Browser tooling for testing
- **Documentation** - Comprehensive READMEs and guides

### ðŸŽ¯ Key Features
- **Single Base Manifests** - No v1/v2 duplication
- **Environment-Centric Structure** - Production, staging, integration
- **Cross-Namespace Routing** - GRPCRoute + ReferenceGrant
- **Automatic Cleanup** - PR namespaces pruned on close
- **Generic Workflows** - Reusable across services
- **Full Observability** - Metrics, logs, and traces

This implementation is **production-ready** and follows modern GitOps practices with comprehensive observability and developer experience.
